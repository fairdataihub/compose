# Ollama Docker Compose for Ubuntu with Snap Docker (NOT RECOMMENDED)
#
# WARNING: This configuration is provided for Ubuntu systems using Snap Docker,
# but we DO NOT recommend using it due to numerous issues with Snap confinement
# and nvidia-container-toolkit.
#
# Known issues with Snap Docker:
# - CDI (Container Device Interface) method doesn't work due to Snap confinement
# - The `gpus:` syntax fails with permission/path errors
# - Requires legacy `runtime: nvidia` workaround
# - Inconsistent behavior across Ubuntu versions
# - Updates can break GPU passthrough unexpectedly
#
# RECOMMENDATION: If possible, uninstall Snap Docker and install Docker via
# the official Docker apt repository instead. Then use ollama-compose.yml.
#
# To switch from Snap Docker to official Docker:
#   sudo snap remove docker
#   # Follow: https://docs.docker.com/engine/install/ubuntu/

services:
  ollama:
    # Official Ollama Docker image - pulls the latest version
    image: ollama/ollama:latest

    # Explicit container name for easier reference and service discovery
    container_name: ollama

    # Automatically restart the container if it stops (unless manually stopped)
    restart: unless-stopped

    # Use nvidia runtime for GPU support (required for Snap Docker)
    # This is the legacy method - prefer gpus: syntax on non-Snap installations
    runtime: nvidia

    environment:
      # Bind Ollama to all network interfaces (0.0.0.0) on port 11434
      # This allows other containers on the Docker network to access the service
      - OLLAMA_HOST=0.0.0.0:11434
      # Enable all GPUs (required when using runtime: nvidia instead of deploy.resources)
      - NVIDIA_VISIBLE_DEVICES=all

    volumes:
      # Persist Ollama data (downloaded models, configuration) in a named volume
      # This ensures models are retained across container restarts and updates
      - ollama:/root/.ollama

    # Connect to the external Docker network for inter-container communication
    # Other services can access this container using the hostname "ollama"
    networks:
      - ollama-network

    # Note: Using 'runtime: nvidia' + 'NVIDIA_VISIBLE_DEVICES=all' instead of 'gpus: all'
    # This is required for Snap Docker with nvidia-container-toolkit
    # The 'gpus: all' syntax uses the CDI method which has Snap confinement issues

    # Health check to monitor service availability
    # Runs "ollama list" command every 30 seconds to verify the service is responding
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 30s          # Check every 30 seconds
      timeout: 10s           # Wait up to 10 seconds for response
      retries: 5             # Mark as unhealthy after 5 consecutive failures
      start_period: 30s      # Allow 30 seconds for initial startup before checking

    # OPTIONAL: Expose Ollama to the host machine
    # Uncomment the lines below if you want to access Ollama from your host
    # (e.g., using curl, Postman, or a local application)
    #
    # WARNING: This will expose port 11434 on your host. If you already have
    # Ollama running on your host machine, this will cause a port conflict.
    # ports:
    #   - "11434:11434"  # Maps container port 11434 to host port 11434

# Named volume for persistent storage
# Stores all Ollama data including downloaded models and configuration
volumes:
  ollama:

# External network configuration
# This network must be created before starting this compose file:
#   docker network create ollama-network
#
# Using an external network allows multiple Docker Compose projects to
# share the same network and communicate with each other
networks:
  ollama-network:
    name: ollama-network
    external: true  # Network exists outside this compose file
